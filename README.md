# ğŸ¥‰ KaggleBronze_AIJC_detector

This project documents my journey in a unique Kaggle competition, where I earned my **first Kaggle medal** â€“ a ğŸ¥‰ **bronze medal**, after participating in many contests!  
æœ¬é¡¹ç›®è®°å½•äº†æˆ‘åœ¨ä¸€åœºç‹¬ç‰¹çš„ Kaggle æ¯”èµ›ä¸­çš„ç»å†ï¼Œè¿™æ˜¯æˆ‘å‚ä¸å¤šåœºæ¯”èµ›åï¼Œè·å¾—çš„ç¬¬ä¸€å— ğŸ¥‰ **é“œç‰Œ**ï¼

---

## ğŸ“˜ Competition Overview / æ¯”èµ›æ¦‚è§ˆ

The dataset provided was highly **imbalanced**:  
å®˜æ–¹æä¾›çš„æ•°æ®é›†éå¸¸ **ä¸å¹³è¡¡**ï¼š

- ğŸ§  **AI-generated texts**: Only 3 available  
  ä»…åŒ…å« 3 ç¯‡ç”± AI ç”Ÿæˆçš„æ–‡æœ¬  
- ğŸ‘¥ **Human-written texts**: A lot  
  è€Œäººç±»æ’°å†™çš„æ–‡æœ¬éå¸¸å¤š

So I used **popular LLMs** to generate **pseudo data** to balance the dataset.  
å› æ­¤æˆ‘ä½¿ç”¨äº†å¤šä¸ªä¸»æµå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆä¼ªæ•°æ®ï¼Œè¡¥è¶³ AI æ ·æœ¬æ•°é‡ã€‚

---

## âš ï¸ Competition Challenges / ç«èµ›éš¾ç‚¹

### 1. ğŸª¤ Artificial Spelling Errors / äººä¸ºæ‹¼å†™é”™è¯¯

Organizers **manually altered** AI texts and added **spelling mistakes**.  
ä¸»åŠæ–¹ **äººä¸ºä¿®æ”¹** äº† AI æ–‡æœ¬å¹¶æ·»åŠ äº†æ‹¼å†™é”™è¯¯ã€‚

- CV score was **0.99**, but public leaderboard only **0.8**  
  æœ¬åœ°äº¤å‰éªŒè¯å¯è¾¾ 0.99ï¼Œæ’è¡Œæ¦œå´ä»…æœ‰ 0.8

---

### 2. ğŸ•µï¸â€â™‚ï¸ Prompt Trap / Prompt é™·é˜±

- Train set had **2 prompts**, public test had **5 unknown**  
  è®­ç»ƒé›†ä»…åŒ…å« 2 ä¸ªæç¤ºè¯ï¼Œè€Œæµ‹è¯•é›†åŒ…å« **5 ä¸ªæœªçŸ¥ prompt**
- Required guessing & mining via uploads  
  éœ€è¦é€‰æ‰‹é€šè¿‡ä¸Šä¼ æµ‹è¯•ç»“æœæ¥åæ¨å‡ºè¿™äº› prompt

---

### 3. ğŸ§  Overfitting Transformers / Transformer è¿‡æ‹Ÿåˆ

- BERT, deBERTa performed poorly  
  ä¸»æµæ¨¡å‹å¦‚ BERTã€deBERTa æ•ˆæœå¾ˆå·®
- Likely failed due to misspellings  
  åŸå› æ˜¯æ— æ³•é€‚åº”æ‹¼å†™é”™è¯¯æ‰°åŠ¨

---

## ğŸ”§ My Solution Pipeline / æˆ‘çš„è§£å†³æ–¹æ¡ˆ

- Limited to **one notebook** (train + inference)  
  æ¯”èµ›ä»…å…è®¸ä¸€ä¸ª notebook åŒ…å«è®­ç»ƒå’Œæ¨ç†

- Used `leven_search` to build **sentence_correcter()**  
  åˆ©ç”¨ `leven_search` åº“æ„å»ºå‡½æ•°çº æ­£æ‹¼å†™é”™è¯¯  
  ç§»é™¤äº†å¤§é‡å™ªå£°å’Œå¹²æ‰°

---

## ğŸ§ª Data Collection / æ•°æ®æ”¶é›†ä¸æ¸…æ´—

- Sourced data from Kaggle discussion  
  è®­ç»ƒæ•°æ®æ¥è‡ªè®¨è®ºåŒºåˆ†äº«  
- âœ… Deduplicated & normalized  
  è¿›è¡Œå»é‡ä¸æ ‡å‡†åŒ–å¤„ç†

### ğŸ§¼ Normalization specifics / æ ‡å‡†åŒ–ç»†èŠ‚ï¼š

- Removed samples with **known harmful prompts**  
  åˆ é™¤äº†å¸¦æœ‰æœ‰å®³ prompt çš„æ ·æœ¬
- Applied **Unicode NFC**, case sensitivity, BPE tokenizer  
  ä½¿ç”¨ NFC ç»Ÿä¸€å­—ç¬¦ç¼–ç ã€è‡ªå»º BPE åˆ†è¯å™¨ç­‰

---

## ğŸ§  Tokenization Strategy / åˆ†è¯ç­–ç•¥

Built a custom tokenizer using **Byte-Pair Encoding (BPE)**  
æˆ‘è‡ªå»ºäº†ä¸€ä¸ª BPE åˆ†è¯å™¨ï¼Œä¸“é—¨åº”å¯¹æ‹¼å†™é”™ä¹±é—®é¢˜ã€‚

- Avoided using default tokenizers  
  é¿å…ä½¿ç”¨é€šç”¨ tokenizer æ··æ·†é”™åˆ«å­—

---

## ğŸ”¢ TF-IDF Vectorization / å‘é‡åŒ–

- Assigned each token a TF-IDF score  
  æ¯ä¸ª token é€šè¿‡ TF-IDF æ˜ å°„æˆå‘é‡  
- Used `min_df = 0` to keep rare typo words  
  è®¾ç½® `min_df = 0`ï¼Œä¿ç•™æ‹¼å†™é”™è¯¯ç­‰ç¨€æœ‰è¯

---

## ğŸ¤– Models Used / æ¨¡å‹ä½¿ç”¨

Trained and ensembled:  
æˆ‘è®­ç»ƒå¹¶èåˆäº†ä»¥ä¸‹æ¨¡å‹ï¼š

- ğŸ“Š Naive Bayes
- âš™ï¸ SGD Classifier
- ğŸ”¥ LightGBM Classifier

Used **weighted voting** for final prediction.  
æœ€ç»ˆç»“æœä½¿ç”¨åŠ æƒæŠ•ç¥¨èåˆè¾“å‡ºã€‚

---

## ğŸ† Result / æ¯”èµ›ç»“æœ

Out of **2,175 teams**, I ranked in the **top 9%**  
åœ¨ **2,175 æ”¯é˜Ÿä¼** ä¸­ï¼Œæˆ‘æˆåŠŸè¿›å…¥äº†å‰ 9% ğŸ¥³

å¹¶è·å¾—äº†äººç”Ÿç¬¬ä¸€æš Kaggle é“œç‰Œ ğŸ¥‰

---

## ğŸ™ Final Thoughts / æœ€åæ„Ÿæƒ³

Through this challenge, I practiced:  
é€šè¿‡æœ¬æ¬¡æ¯”èµ›ï¼Œæˆ‘æ·±å…¥äº†è§£å¹¶å®è·µäº†ï¼š

- LLM data augmentation ğŸ¤– å¤§æ¨¡å‹ç”Ÿæˆä¼ªæ•°æ®
- Adversarial robustness ğŸ§¨ æŠ—å¹²æ‰°å»ºæ¨¡èƒ½åŠ›
- Model generalization ğŸ“‰ æ¨¡å‹æ³›åŒ–ä¸æ’è¡Œæ¦œå·®è·

Thanks for reading! æ¬¢è¿äº¤æµï¼
